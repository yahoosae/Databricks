{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34dfa20f-b20e-433e-a8f7-f43a35b5b8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b25db98-7dce-429e-9639-4a5dad58fb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Workspace/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af7e5a4-fc69-4eb5-a533-379d4f053a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DataBricks File System (DBFS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36819877-8e81-4f05-ac14-bf2983c563b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "#Create directory\n",
    "dbutils.fs.mkdirs(\"/Workspace/foobar\")\n",
    "# dbutils.fs.ls(\"/Workspace/foobar/\")\n",
    "\n",
    " #Create file and write\n",
    "dbutils.fs.put(\"/Workspace/foobar/baz.txt\", \"Hello World!\", overwrite=True)\n",
    "\n",
    " #Reading file\n",
    "# dbutils.fs.head(\"/Workspace/foobar/baz.txt\")\n",
    "\n",
    " #Access file path\n",
    "# display(dbutils.fs.ls(\"dbfs:/Workspace/foobar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79254e82-9ef9-41b7-afc1-fab572874748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creare parquet file \n",
    "#Create DataFrame df1 with columns name,dept & age\n",
    "data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n",
    "    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
    "\n",
    "columns= [\"name\",\"dept\",\"age\"]\n",
    "dataframe1 = spark.createDataFrame(data = data, schema = columns)\n",
    "dataframe1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a205c2da-93d2-47af-830a-0a5375009000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe1.write.mode('overwrite').parquet('dbfs:/Workspace/tmp/output/people3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3953a329-a813-4a2a-a646-39ba426cac4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/people3.parquet/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c79898-0e55-498c-ac16-d9dad055ff34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe1.write.mode(\"append\").partitionBy(\"age\").parquet(\"dbfs:/Workspace/tmp/output/first.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a941a60-a331-494f-a29d-87bab21e6c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/first.parquet/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/first.parquet/age=24/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39892bf5-f30b-4e96-b1fb-ec86131db336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read\n",
    "df=spark.read.parquet(\"dbfs:/Workspace/tmp/output/people3.parquet/\")\n",
    "#df=spark.read.parquet(\"dbfs:/Workspace/tmp/output/people3.parquet/age=24/\")#sometime works and sometime not\n",
    "#df.show()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92320719-8ac8-4a64-8a25-d12c6ecb6319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Append\n",
    "\n",
    "#Create another one\n",
    "data1 = [(\"James1\",\"Sales1\",34), (\"Michael1\",\"Sales\",56), \\\n",
    "    (\"Robert1\",\"Sales1\",30), (\"Maria1\",\"Finance\",24) ]\n",
    "\n",
    "columns1= [\"name\",\"dept\",\"age\"]\n",
    "dataframe2 = spark.createDataFrame(data = data1, schema = columns1)\n",
    "\n",
    "dataframe2.write.mode('append').parquet('dbfs:/Workspace/tmp/output/people3.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28879cf4-8e1b-4254-ae72-9b98cd357ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data2 = [(2012,8,\"Batman\",9.8),\n",
    "           (2012,8,\"Hero\",8.7),\n",
    "           (2012,7,\"Robot\",5.5),\n",
    "           (2011,7,\"git\",2.0)\n",
    "  ]\n",
    "\n",
    "columns = [\"year\",\"month\",\"title\",\"rating\"]\n",
    "df = spark.createDataFrame(data=data2,schema = columns)\n",
    "\n",
    "df.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").format(\"avro\").save(\"dbfs:/Workspace/tmp/output/tmp/test_Dataset01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef152ae-a69e-4d0b-9997-95fdf29802d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/tmp/test_Dataset01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20a5cb6-a720-4081-91f9-6d21aec62f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.read.format(\"avro\").load(\"dbfs:/Workspace/tmp/output/tmp/test_Dataset01\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855e719b-ba7f-4d0e-b29b-72db773d2843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List\n",
    "data = [{\n",
    "    'col1': 'Category A',\n",
    "    'col2': 100\n",
    "}, {\n",
    "    'col1': 'Category B',\n",
    "    'col2': 200\n",
    "}, {\n",
    "    'col1': 'Category C',\n",
    "    'col2': 300\n",
    "}]\n",
    "\n",
    "\n",
    "# Save as Orc\n",
    "df.write.format('orc').mode('overwrite').save(\n",
    "    'dbfs:/Workspace/tmp/FileStore/tables/userdata1_orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5614c4a5-aae3-41c7-b309-e93a6b06d39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"dbfs:/Workspace/tmp/FileStore/tables/userdata1_orc\"\n",
    "file_type = \"orc\"\n",
    "\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .load(file_location)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558c6437-ea42-4b93-9698-ac522b77a65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create large dataset\n",
    "df = spark.range(0, 10_000)\n",
    "df_transformed = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])\n",
    "\n",
    "# First action: count()\n",
    "start = time.time()\n",
    "df_transformed.count()\n",
    "print(\"Without Cache - First count done in:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# Second action: sum()\n",
    "start = time.time()\n",
    "df_transformed.agg(F.sum(\"square\")).show()\n",
    "print(\"Without Cache - Second aggregation done in:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17cb8b16-3083-4fc7-8f33-8c6f0f546161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#With cache\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create large dataset\n",
    "df = spark.range(0, 10_000)\n",
    "df_transformed = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])\n",
    "# Apply caching\n",
    "df_transformed.cache()\n",
    "# First action: count() triggers caching\n",
    "start = time.time()\n",
    "df_transformed.count() #first action is slower (because it's doing both compute + caching).\n",
    "print(\"With Cache - First count done in:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# Second action: sum() uses cached data\n",
    "start = time.time()\n",
    "df_transformed.agg(F.sum(\"square\")).show()\n",
    "print(\"With Cache - Second aggregation done in:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc001b8-5e9e-4142-8990-e65fef3a86ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_transformed.is_cached\n",
    "     \n",
    "\n",
    "df_transformed.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f066fbec-8841-4667-85bf-05994818eb78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Practice Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0337cf6c-b1df-477d-a463-85db3729cef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------- Customers ----------------\n",
    "customers_data = [\n",
    "    (1, \"Amit\", \"Delhi\", \"Active\"),\n",
    "    (2, \"Neha\", \"Mumbai\", \"Active\"),\n",
    "    (3, \"Raj\", \"Delhi\", \"Inactive\"),\n",
    "    (4, \"Simran\", \"Bangalore\", \"Active\"),\n",
    "    (5, \"John\", \"Chennai\", \"Active\")\n",
    "]\n",
    "\n",
    "customers = spark.createDataFrame(\n",
    "    customers_data,\n",
    "    [\"customer_id\",\"name\",\"city\",\"status\"]\n",
    ")\n",
    "\n",
    "# ---------------- Orders ----------------\n",
    "orders_data = [\n",
    "    (101,1,1000.0,\"2024-01-01\"),\n",
    "    (102,2,None,\"2024-01-02\"),\n",
    "    (103,1,-50.0,\"2024-01-03\"),\n",
    "    (104,3,500.0,\"2024-01-04\"),\n",
    "    (105,4,700.0,\"2024-01-05\"),\n",
    "    (106,5,1200.0,\"2024-01-06\")\n",
    "]\n",
    "\n",
    "orders = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"order_id\",\"customer_id\",\"amount\",\"order_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2c1850-ee2a-4f49-9668-b285b3d9f99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers.write.mode(\"overwrite\").partitionBy(\"customer_id\").format(\"csv\").save(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/customers\")\n",
    "orders.write.mode(\"overwrite\").partitionBy(\"order_id\").format(\"csv\").save(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f680e5-1fdb-4eae-93e6-77ba47fa0196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/customers.csv\", \"\", True)\n",
    "dbutils.fs.put(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/orders.csv\", \"\", True)\n",
    "\n",
    "customers.write.csv(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/customers.csv\", mode=\"overwrite\")\n",
    "orders.write.csv(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/orders.csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feab3942-fe46-4707-9a2f-b81ce2b76d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4\n",
    "dbutils.fs.mkdirs(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5797a474-209c-4202-b4b7-4ae3dad9aca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Read the stored files again into new DataFrames\n",
    "\n",
    "customers_new = spark.read.csv(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/customers.csv\", schema=customers.schema)\n",
    "orders_new = spark.read.csv(\"/Workspace/Users/d30930606@gmail.com/Databricks/data/orders.csv\", schema=orders.schema)\n",
    "\n",
    "# 6. Display the DataFrames\n",
    "display(customers_new)\n",
    "display(orders_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edbac3d4-bd33-469a-af84-7247b78a9917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 2 — EXTRACT TASK (Instructions Only)\n",
    "\n",
    "1. Store both datasets into DBFS as raw files.\n",
    "2. Save the customers dataset in CSV format with header.\n",
    "3. Save the orders dataset in CSV format with header.\n",
    "4. Create a raw data folder structure inside DBFS.\n",
    "5. Read the stored files again into new DataFrames.\n",
    "\n",
    "### STEP 3 — TRANSFORM TASK (Instructions Only)\n",
    "\n",
    "1. Perform the following transformations:\n",
    "  - Clean the amount column:\n",
    "    - Replace NULL values with 0.\n",
    "    - Replace negative values with 10.\n",
    "    - Keep remaining values unchanged.\n",
    "\n",
    "2. Create a new column named final_amount by adding 18% GST.\n",
    "3. Filter only customers whose status is Active.\n",
    "4. oin orders data with active customers using customer_id.\n",
    "5. Select only meaningful business columns:\n",
    "    - customer name\n",
    "    - city\n",
    "    - order_id\n",
    "    - final_amount\n",
    "    - order_date\n",
    "\n",
    "6. Calculate total order amount customer-wise.\n",
    "7. Rank customers inside each city based on final_amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060ad6c7-d72a-42e0-a157-7d845cf982d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP-3: TRANSFORMATIONS\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# 1\n",
    "orders_transformed = orders_new.fillna(0)\n",
    "# Replace negative values with 10.\n",
    "orders_transformed = orders_transformed.withColumn(\"amount\", F.when(F.col(\"amount\") < 0, 10).otherwise(F.col(\"amount\")))\n",
    "display(orders_transformed)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "My Databricks Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
