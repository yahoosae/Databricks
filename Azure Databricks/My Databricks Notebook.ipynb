{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34dfa20f-b20e-433e-a8f7-f43a35b5b8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark)\n",
    "base_path = r\"/Workspace/Users/d30930606@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b25db98-7dce-429e-9639-4a5dad58fb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{base_path}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af7e5a4-fc69-4eb5-a533-379d4f053a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DataBricks File System (DBFS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36819877-8e81-4f05-ac14-bf2983c563b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "#Create directory\n",
    "dbutils.fs.mkdirs(f\"{base_path}/foobar\")\n",
    "# dbutils.fs.ls(\"/Workspace/foobar/\")\n",
    "\n",
    " #Create file and write\n",
    "# dbutils.fs.put(f\"{base_path}/foobar/baz.txt\", \"Hello World!\", overwrite=True)\n",
    "\n",
    " #Reading file\n",
    "# dbutils.fs.head(f\"{base_path}/foobar/baz.txt\")\n",
    "\n",
    " #Access file path\n",
    "display(dbutils.fs.ls(f\"{base_path}/foobar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79254e82-9ef9-41b7-afc1-fab572874748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creare parquet file \n",
    "#Create DataFrame df1 with columns name,dept & age\n",
    "data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n",
    "    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
    "\n",
    "columns= [\"name\",\"dept\",\"age\"]\n",
    "dataframe1 = spark.createDataFrame(data = data, schema = columns)\n",
    "dataframe1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a205c2da-93d2-47af-830a-0a5375009000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe1.write.mode('overwrite').parquet(f'{base_path}/trial/people3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3953a329-a813-4a2a-a646-39ba426cac4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/people3.parquet/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c79898-0e55-498c-ac16-d9dad055ff34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe1.write.mode(\"append\").partitionBy(\"age\").parquet(\"dbfs:/Workspace/tmp/output/first.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a941a60-a331-494f-a29d-87bab21e6c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/first.parquet/\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/first.parquet/age=24/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39892bf5-f30b-4e96-b1fb-ec86131db336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read\n",
    "df=spark.read.parquet(\"dbfs:/Workspace/tmp/output/people3.parquet/\")\n",
    "#df=spark.read.parquet(\"dbfs:/Workspace/tmp/output/people3.parquet/age=24/\")#sometime works and sometime not\n",
    "#df.show()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92320719-8ac8-4a64-8a25-d12c6ecb6319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Append\n",
    "\n",
    "#Create another one\n",
    "data1 = [(\"James1\",\"Sales1\",34), (\"Michael1\",\"Sales\",56), \\\n",
    "    (\"Robert1\",\"Sales1\",30), (\"Maria1\",\"Finance\",24) ]\n",
    "\n",
    "columns1= [\"name\",\"dept\",\"age\"]\n",
    "dataframe2 = spark.createDataFrame(data = data1, schema = columns1)\n",
    "\n",
    "dataframe2.write.mode('append').parquet('dbfs:/Workspace/tmp/output/people3.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28879cf4-8e1b-4254-ae72-9b98cd357ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data2 = [(2012,8,\"Batman\",9.8),\n",
    "           (2012,8,\"Hero\",8.7),\n",
    "           (2012,7,\"Robot\",5.5),\n",
    "           (2011,7,\"git\",2.0)\n",
    "  ]\n",
    "\n",
    "columns = [\"year\",\"month\",\"title\",\"rating\"]\n",
    "df = spark.createDataFrame(data=data2,schema = columns)\n",
    "\n",
    "df.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").format(\"avro\").save(\"dbfs:/Workspace/tmp/output/tmp/test_Dataset01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef152ae-a69e-4d0b-9997-95fdf29802d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/Workspace/tmp/output/tmp/test_Dataset01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20a5cb6-a720-4081-91f9-6d21aec62f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.read.format(\"avro\").load(\"dbfs:/Workspace/tmp/output/tmp/test_Dataset01\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855e719b-ba7f-4d0e-b29b-72db773d2843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List\n",
    "data = [{\n",
    "    'col1': 'Category A',\n",
    "    'col2': 100\n",
    "}, {\n",
    "    'col1': 'Category B',\n",
    "    'col2': 200\n",
    "}, {\n",
    "    'col1': 'Category C',\n",
    "    'col2': 300\n",
    "}]\n",
    "\n",
    "\n",
    "# Save as Orc\n",
    "df.write.format('orc').mode('overwrite').save(\n",
    "    'dbfs:/Workspace/tmp/FileStore/tables/userdata1_orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5614c4a5-aae3-41c7-b309-e93a6b06d39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"dbfs:/Workspace/tmp/FileStore/tables/userdata1_orc\"\n",
    "file_type = \"orc\"\n",
    "\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .load(file_location)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558c6437-ea42-4b93-9698-ac522b77a65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create large dataset\n",
    "df = spark.range(0, 10_000)\n",
    "df_transformed = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])\n",
    "\n",
    "# First action: count()\n",
    "start = time.time()\n",
    "df_transformed.count()\n",
    "print(\"Without Cache - First count done in:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# Second action: sum()\n",
    "start = time.time()\n",
    "df_transformed.agg(F.sum(\"square\")).show()\n",
    "print(\"Without Cache - Second aggregation done in:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17cb8b16-3083-4fc7-8f33-8c6f0f546161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#With cache\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create large dataset\n",
    "df = spark.range(0, 10_000)\n",
    "df_transformed = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])\n",
    "# Apply caching\n",
    "df_transformed.cache()\n",
    "# First action: count() triggers caching\n",
    "start = time.time()\n",
    "df_transformed.count() #first action is slower (because it's doing both compute + caching).\n",
    "print(\"With Cache - First count done in:\", time.time() - start, \"seconds\")\n",
    "\n",
    "# Second action: sum() uses cached data\n",
    "start = time.time()\n",
    "df_transformed.agg(F.sum(\"square\")).show()\n",
    "print(\"With Cache - Second aggregation done in:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc001b8-5e9e-4142-8990-e65fef3a86ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_transformed.is_cached\n",
    "     \n",
    "\n",
    "df_transformed.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f066fbec-8841-4667-85bf-05994818eb78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Practice Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0337cf6c-b1df-477d-a463-85db3729cef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------- Customers ----------------\n",
    "customers_data = [\n",
    "    (1, \"Amit\", \"Delhi\", \"Active\"),\n",
    "    (2, \"Neha\", \"Mumbai\", \"Active\"),\n",
    "    (3, \"Raj\", \"Delhi\", \"Inactive\"),\n",
    "    (4, \"Simran\", \"Bangalore\", \"Active\"),\n",
    "    (5, \"John\", \"Chennai\", \"Active\")\n",
    "]\n",
    "\n",
    "customers = spark.createDataFrame(\n",
    "    customers_data,\n",
    "    [\"customer_id\",\"name\",\"city\",\"status\"]\n",
    ")\n",
    "\n",
    "# ---------------- Orders ----------------\n",
    "orders_data = [\n",
    "    (101,1,1000.0,\"2024-01-01\"),\n",
    "    (102,2,None,\"2024-01-02\"),\n",
    "    (103,1,-50.0,\"2024-01-03\"),\n",
    "    (104,3,500.0,\"2024-01-04\"),\n",
    "    (105,4,700.0,\"2024-01-05\"),\n",
    "    (106,5,1200.0,\"2024-01-06\")\n",
    "]\n",
    "\n",
    "orders = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"order_id\",\"customer_id\",\"amount\",\"order_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2c1850-ee2a-4f49-9668-b285b3d9f99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers.write.mode(\"overwrite\").partitionBy(\"customer_id\").format(\"csv\").save(\"{base_path}/Databricks/data/customers\")\n",
    "orders.write.mode(\"overwrite\").partitionBy(\"order_id\").format(\"csv\").save(\"{base_path}/Databricks/data/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f680e5-1fdb-4eae-93e6-77ba47fa0196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"{base_path}/Databricks/data/customers.csv\", \"\", True)\n",
    "dbutils.fs.put(\"{base_path}/Databricks/data/orders.csv\", \"\", True)\n",
    "\n",
    "customers.write.csv(\"{base_path}/Databricks/data/customers.csv\", mode=\"overwrite\")\n",
    "orders.write.csv(\"{base_path}/Databricks/data/orders.csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feab3942-fe46-4707-9a2f-b81ce2b76d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4\n",
    "dbutils.fs.mkdirs(\"{base_path}/Databricks/data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5797a474-209c-4202-b4b7-4ae3dad9aca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Read the stored files again into new DataFrames\n",
    "\n",
    "customers_new = spark.read.csv(\"{base_path}/Databricks/data/customers.csv\", schema=customers.schema)\n",
    "orders_new = spark.read.csv(\"{base_path}/Databricks/data/orders.csv\", schema=orders.schema)\n",
    "\n",
    "# 6. Display the DataFrames\n",
    "display(customers_new)\n",
    "display(orders_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edbac3d4-bd33-469a-af84-7247b78a9917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 2 — EXTRACT TASK (Instructions Only)\n",
    "\n",
    "1. Store both datasets into DBFS as raw files.\n",
    "2. Save the customers dataset in CSV format with header.\n",
    "3. Save the orders dataset in CSV format with header.\n",
    "4. Create a raw data folder structure inside DBFS.\n",
    "5. Read the stored files again into new DataFrames.\n",
    "\n",
    "### STEP 3 — TRANSFORM TASK (Instructions Only)\n",
    "\n",
    "1. Perform the following transformations:\n",
    "  - Clean the amount column:\n",
    "    - Replace NULL values with 0.\n",
    "    - Replace negative values with 10.\n",
    "    - Keep remaining values unchanged.\n",
    "\n",
    "2. Create a new column named final_amount by adding 18% GST.\n",
    "3. Filter only customers whose status is Active.\n",
    "4. oin orders data with active customers using customer_id.\n",
    "5. Select only meaningful business columns:\n",
    "    - customer name\n",
    "    - city\n",
    "    - order_id\n",
    "    - final_amount\n",
    "    - order_date\n",
    "\n",
    "6. Calculate total order amount customer-wise.\n",
    "7. Rank customers inside each city based on final_amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060ad6c7-d72a-42e0-a157-7d845cf982d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP-3: TRANSFORMATIONS\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# 1\n",
    "orders_transformed = orders_new.fillna(0)\n",
    "# Replace negative values with 10.\n",
    "orders_transformed = orders_transformed.withColumn(\"amount\", F.when(F.col(\"amount\") < 0, 10).otherwise(F.col(\"amount\")))\n",
    "display(orders_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5e1868-9f0f-4381-82d7-ceeed97465af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2. Create final_amount with 18% GST"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Create final_amount column by adding 18% GST\n",
    "orders_transformed = orders_transformed.withColumn(\n",
    "    \"final_amount\", \n",
    "    F.col(\"amount\") * 1.18\n",
    ")\n",
    "\n",
    "display(orders_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e74c1b6c-4932-41ac-bf92-f84e791ecdc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3. Filter active customers"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Filter only customers whose status is Active\n",
    "active_customers = customers_new.filter(F.col(\"status\") == \"Active\")\n",
    "\n",
    "display(active_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c8cc0e-2820-4f49-81ce-2254f2681c84",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4. Join orders with active customers"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Join orders data with active customers using customer_id\n",
    "joined_data = orders_transformed.join(\n",
    "    active_customers,\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "display(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03b15e5c-4105-428d-9299-33451cd95a26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "5. Select meaningful business columns"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Select only meaningful business columns\n",
    "business_data = joined_data.select(\n",
    "    F.col(\"name\").alias(\"customer_name\"),\n",
    "    \"city\",\n",
    "    \"order_id\",\n",
    "    \"final_amount\",\n",
    "    \"order_date\"\n",
    ")\n",
    "\n",
    "display(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f9129f-d7ea-4db2-af47-84edf4a6d1d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "6. Calculate total order amount per customer"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Calculate total order amount customer-wise\n",
    "customer_totals = business_data.groupBy(\"customer_name\", \"city\").agg(\n",
    "    F.sum(\"final_amount\").alias(\"total_order_amount\")\n",
    ").orderBy(F.desc(\"total_order_amount\"))\n",
    "\n",
    "display(customer_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "972aa939-2714-4c35-9c6f-bdab10bb7d2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "7. Rank customers by city based on final_amount"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Rank customers inside each city based on final_amount\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create window partitioned by city, ordered by total amount descending\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(F.desc(\"total_order_amount\"))\n",
    "\n",
    "# Add rank column\n",
    "ranked_customers = customer_totals.withColumn(\n",
    "    \"rank_in_city\",\n",
    "    F.rank().over(window_spec)\n",
    ").orderBy(\"city\", \"rank_in_city\")\n",
    "\n",
    "display(ranked_customers)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "My Databricks Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
